# --- .github/workflows/main.yml ---
# 這是 GitHub Actions 的設定檔
# 它會：
# 1. 每天早上 6 點 (台灣時間) 自動執行
# 2. 也可以手動執行 (workflow_dispatch)
# 3. 執行 scrape.py 腳本
# 4. 自動將產生的 data.json 存回儲存庫

name: Law Scraper Workflow

on:
  # 1. 允許您在 GitHub 頁面上手動觸發此流程 (用於測試)
  workflow_dispatch:
  
  # 2. 每天 22:00 UTC (世界標準時間) 自動執行
  #    這大約是台灣時間 (UTC+8) 的早上 6 點
  schedule:
    - cron: "0 22 * * *"

jobs:
  scrape-and-commit:
    runs-on: ubuntu-latest # 使用最新的 Ubuntu 虛擬機

    steps:
      # 步驟 1: 取得您儲存庫的程式碼
      - name: Checkout repository
        uses: actions/checkout@v4

      # 步驟 2: 設定 Python 環境 (使用 3.10 版本)
      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      # 步驟 3: 安裝我們需要的函式庫 (即 requirements.txt 裡的 "requests")
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      # 步驟 4: 執行我們的 Python 爬蟲腳本
      # 這會產生 data.json 檔案
      - name: Run scraper
        run: python scrape.py

      # 步驟 5: 自動將 data.json 檔案「提交」回儲存庫
      - name: Commit data.json
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Auto: Update data.json (自動更新法規資料)"
          file_pattern: data.json # 只提交 data.json 這一個檔案
          commit_user_name: "GitHub Actions Bot"
          commit_user_email: "actions-bot@github.com"
          commit_author: "GitHub Actions Bot <actions-bot@github.com>"
